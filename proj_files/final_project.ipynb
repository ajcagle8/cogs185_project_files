{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd08795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dlib\n",
      "  Using cached dlib-19.24.4.tar.gz (3.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (PEP 517): started\n",
      "  Building wheel for dlib (PEP 517): finished with status 'error'\n",
      "Failed to build dlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\AJCag\\anaconda3\\python.exe' 'C:\\Users\\AJCag\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\AJCag\\AppData\\Local\\Temp\\tmpmsf7w_e6'\n",
      "       cwd: C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-install-62hcn41w\\dlib_6563e946da8345df8ca1da74c7609c49\n",
      "  Complete output (73 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  <string>:125: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  Building extension for Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n",
      "  Invoking CMake setup: 'cmake C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-install-62hcn41w\\dlib_6563e946da8345df8ca1da74c7609c49\\tools\\python -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-install-62hcn41w\\dlib_6563e946da8345df8ca1da74c7609c49\\build\\lib.win-amd64-cpython-39 -DPYTHON_EXECUTABLE=C:\\Users\\AJCag\\anaconda3\\python.exe -DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-install-62hcn41w\\dlib_6563e946da8345df8ca1da74c7609c49\\build\\lib.win-amd64-cpython-39 -A x64'\n",
      "  -- Building for: NMake Makefiles\n",
      "  CMake Error at CMakeLists.txt:5 (message):\n",
      "  \n",
      "  \n",
      "  \n",
      "    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "  \n",
      "  \n",
      "    You must use Visual Studio to build a python extension on windows.  If you\n",
      "    are getting this error it means you have not installed Visual C++.  Note\n",
      "    that there are many flavors of Visual Studio, like Visual Studio for C#\n",
      "    development.  You need to install Visual Studio for C++.\n",
      "  \n",
      "  \n",
      "    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\AJCag\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 349, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\AJCag\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 331, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"C:\\Users\\AJCag\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 248, in build_wheel\n",
      "      return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 410, in build_wheel\n",
      "      return self._build_with_temp_dir(\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 395, in _build_with_temp_dir\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
      "      exec(code, locals())\n",
      "    File \"<string>\", line 220, in <module>\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 103, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 184, in setup\n",
      "      return run_commands(dist)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 200, in run_commands\n",
      "      dist.run_commands()\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 968, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\wheel\\bdist_wheel.py\", line 368, in run\n",
      "      self.run_command(\"build\")\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 316, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 968, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 132, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 316, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 968, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\AJCag\\AppData\\Local\\Temp\\pip-build-env-1q3wce4e\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"<string>\", line 130, in run\n",
      "    File \"<string>\", line 167, in build_extension\n",
      "    File \"C:\\Users\\AJCag\\anaconda3\\lib\\subprocess.py\", line 373, in check_call\n",
      "      raise CalledProcessError(retcode, cmd)\n",
      "  subprocess.CalledProcessError: Command '['cmake', 'C:\\\\Users\\\\AJCag\\\\AppData\\\\Local\\\\Temp\\\\pip-install-62hcn41w\\\\dlib_6563e946da8345df8ca1da74c7609c49\\\\tools\\\\python', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\\\Users\\\\AJCag\\\\AppData\\\\Local\\\\Temp\\\\pip-install-62hcn41w\\\\dlib_6563e946da8345df8ca1da74c7609c49\\\\build\\\\lib.win-amd64-cpython-39', '-DPYTHON_EXECUTABLE=C:\\\\Users\\\\AJCag\\\\anaconda3\\\\python.exe', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\\\Users\\\\AJCag\\\\AppData\\\\Local\\\\Temp\\\\pip-install-62hcn41w\\\\dlib_6563e946da8345df8ca1da74c7609c49\\\\build\\\\lib.win-amd64-cpython-39', '-A', 'x64']' returned non-zero exit status 1.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for dlib\n",
      "ERROR: Could not build wheels for dlib which use PEP 517 and cannot be installed directly\n"
     ]
    }
   ],
   "source": [
    "#!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8c0998",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import dlib\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import timeit\n",
    "import pickle\n",
    "import random\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to read OCR dataset\n",
    "def l2i(a):\n",
    "    return int(ord(a) - ord('a'))\n",
    "\n",
    "def i2l(i):\n",
    "    if i >= 0:\n",
    "        return chr(i + ord('a'))\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "def iors(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_OCR(filename, n_features):\n",
    "    with open(filename) as F:\n",
    "        dataset = {\n",
    "            'ids': [],\n",
    "            'labels': [],\n",
    "            'labelDic': {},\n",
    "            'next_ids': [],\n",
    "            'word_ids': [],\n",
    "            'positions': [],\n",
    "            'folds': [],\n",
    "            'features': []\n",
    "        }\n",
    "        for str_line in F.readlines():\n",
    "            line0 = list(map(iors, filter(None, re.split('\\t', str_line.strip()))))\n",
    "            dataset['ids'].append(int(line0.pop(0)))\n",
    "            dataset['labels'].append(l2i(line0.pop(0)))\n",
    "            if dataset['labels'][-1] in dataset['labelDic']:\n",
    "                dataset['labelDic'][dataset['labels'][-1]] += 1\n",
    "            else:\n",
    "                dataset['labelDic'][dataset['labels'][-1]] = 1\n",
    "            dataset['next_ids'].append(int(line0.pop(0)))\n",
    "            dataset['word_ids'].append(int(line0.pop(0)))\n",
    "            dataset['positions'].append(int(line0.pop(0)))\n",
    "            dataset['folds'].append(int(line0.pop(0)))\n",
    "            dataset['features'].append(line0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OCR dataset\n",
    "d = 128\n",
    "dataset1 = read_OCR('OCRdataset/letter.data', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6dd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrates OCR raw data\n",
    "print(\"max of labels=\", max(dataset1['labels']), \" min of labels=\", min(dataset1['labels']), 'num of labels=', len(dataset1['labelDic']))\n",
    "print(\"labelDic.keys()=\", list(map(i2l, dataset1['labelDic'].keys())))\n",
    "print(\"Total number of lines=\", len(dataset1['ids']))\n",
    "print(\"The shape of features:\", np.array(dataset1['features']).shape)\n",
    "\n",
    "print(\"The first 10 ids:\", dataset1['ids'][:10])\n",
    "print(\"ids[0]=\", dataset1['ids'][0])\n",
    "print(\"labels[0]=\", dataset1['labels'][0])\n",
    "print(\"The 1st letter is \", i2l(dataset1['labels'][0]))\n",
    "print(\"next_ids[0]=\", dataset1['next_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the matrix into an image\n",
    "def showFeatures(features, num):\n",
    "    plt.figure(figsize=(num, 6))\n",
    "    for i in range(num):\n",
    "        npfeature = np.array(features[i])\n",
    "        plt.subplot(1, num, i + 1)\n",
    "        imshow(npfeature.reshape(16, 8), cmap='gray')\n",
    "        plt.title(i)\n",
    "\n",
    "showFeatures(dataset1['features'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5900c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structurize(dataset, N, L):\n",
    "    d_features = len(dataset['features'][0])\n",
    "    y = dataset['labels']\n",
    "    X = dataset['features']\n",
    "    next_id = dataset['next_ids']\n",
    "\n",
    "    labels = np.zeros((N, L))\n",
    "    features = np.zeros((N, L * d_features))\n",
    "\n",
    "    # Extract only one structured example\n",
    "    def extract(iN, loc):\n",
    "        labels[iN] = y[loc:loc + L]\n",
    "        features[iN] = np.array(X[loc:loc + L]).ravel().tolist()\n",
    "        iN += 1\n",
    "        return iN\n",
    "\n",
    "    iN = 0\n",
    "    iN = extract(iN, 0)\n",
    "\n",
    "    for key, value in enumerate(y):\n",
    "        if next_id[key] == -1:\n",
    "            iN = extract(iN, key + 1)\n",
    "            if iN == N:\n",
    "                break\n",
    "\n",
    "    c = list(zip(labels, features))\n",
    "    random.shuffle(c)\n",
    "    labels, features = zip(*c)\n",
    "\n",
    "    return np.array(labels), np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d27908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeClassProblem:\n",
    "    C = 1\n",
    "\n",
    "    def __init__(self, samples, labels, L, K, d):\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.num_samples = len(samples)\n",
    "        self.num_dimensions = (L * K * d + 1) + (L - 1)\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.loss_for_loop = True\n",
    "\n",
    "    def make_psi(self, x, label):\n",
    "        psi = dlib.vector()\n",
    "        psi.resize(self.num_dimensions)\n",
    "        psi[0] = 1  # The bias term\n",
    "\n",
    "        for i in range(self.L):\n",
    "            for j in range(self.K):\n",
    "                for k in range(self.d):\n",
    "                    psi[i * self.K * self.d + j * self.d + k] = x[i * self.d + k] * (1 if label[i] == j else 0)\n",
    "        for i in range(self.L - 1):\n",
    "            psi[self.L * self.K * self.d + 1 + i] = 1 if label[i] != label[i + 1] else 0\n",
    "\n",
    "        return psi\n",
    "\n",
    "    def get_truth_joint_feature_vector(self, idx):\n",
    "        return self.make_psi(self.samples[idx], self.labels[idx])\n",
    "\n",
    "    def separation_oracle(self, idx, current_solution):\n",
    "        samp = self.samples[idx]\n",
    "        psi = [0] * self.num_dimensions\n",
    "        max1 = -1e10\n",
    "        max_scoring_label = [0] * self.L  # Initialize max_scoring_label for ICM search\n",
    "        for k in range(Niter):\n",
    "            for iL in range(self.L):  # Iterate over the window length\n",
    "                for i in range(self.K):  # Change different label for the search of a structured label\n",
    "                    tmp_label = list(max_scoring_label)  # New a list to avoid modifying the max_scoring_label\n",
    "                    tmp_label[iL] = i  # Take turns to modify the structured label from left to right. The guessed structured label.\n",
    "                    tmp_psi = self.make_psi(samp, tmp_label)  # Make a new Psi for the guessed structured label\n",
    "                    score1 = dlib.dot(current_solution, tmp_psi)\n",
    "\n",
    "                    loss1 = sum([1.0 if self.labels[idx][j] != tmp_label[j] else 0.0 for j in range(self.L)])\n",
    "\n",
    "                    if max1 < score1 + loss1:  # Search for the maximum and update loss, max_scoring_label, and psi\n",
    "                        max1 = score1 + loss1\n",
    "                        loss = loss1\n",
    "                        max_scoring_label[iL] = i\n",
    "                        psi = tmp_psi\n",
    "\n",
    "        return loss, psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredSVM:\n",
    "    def __init__(self, C, L, K, d, Niter):\n",
    "        self.C = C\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.Niter = Niter\n",
    "\n",
    "    def fit(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        problem = ThreeClassProblem(samples, labels, self.L, self.K, self.d)\n",
    "        start_train = timeit.default_timer()\n",
    "        self.weights = dlib.solve_structural_svm_problem(problem)\n",
    "        end_train = timeit.default_timer()\n",
    "        self.train_time = end_train - start_train\n",
    "        print(\"Training time elapsed:\", self.train_time, \"s\")\n",
    "        pickle.dump(self.weights, open('weights_structured_svm.obj', 'wb'))\n",
    "\n",
    "    def predict(self, samples):\n",
    "        predictions = []\n",
    "        problem = ThreeClassProblem(samples, [], self.L, self.K, self.d)\n",
    "        for idx, samp in enumerate(samples):\n",
    "            max1 = -1e10\n",
    "            max_scoring_label = [0] * self.L  # Initialize max_scoring_label for ICM search\n",
    "            for k in range(self.Niter):\n",
    "                for iL in range(self.L):  # Iterate over the window length\n",
    "                    for i in range(self.K):  # Change different label for the search of a structured label\n",
    "                        tmp_label = list(max_scoring_label)  # New a list to avoid modifying the max_scoring_label\n",
    "                        tmp_label[iL] = i  # Take turns to modify the structured label from left to right\n",
    "                        tmp_psi = problem.make_psi(samp, tmp_label)  # Make a new Psi for the guessed structured label\n",
    "                        score1 = dlib.dot(self.weights, tmp_psi)\n",
    "                        if max1 < score1:\n",
    "                            max1 = score1\n",
    "                            max_scoring_label[iL] = i\n",
    "            predictions.append(max_scoring_label)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, samples, labels):\n",
    "        predictions = self.predict(samples)\n",
    "        errCnt = sum([1 if predictions[i] != labels[i] else 0 for i in range(len(predictions))])\n",
    "        return 1.0 - float(errCnt) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f078694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run structured SVM experiments\n",
    "def run_svm_experiment(dataset, window_size, N, d, train_size, test_size):\n",
    "    labels, features = structurize(dataset, N, window_size)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    nplabels = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "    npsamples = np.hstack([np.ones((N, 1)), features])\n",
    "    K = len(le.classes_)\n",
    "\n",
    "    tr_labels = nplabels[:train_size].astype(int).tolist()\n",
    "    tr_samples = npsamples[:train_size].astype(int).tolist()\n",
    "    te_labels = nplabels[train_size:train_size + test_size].astype(int).tolist()\n",
    "    te_samples = npsamples[train_size:train_size + test_size].astype(int).tolist()\n",
    "\n",
    "    model = StructuredSVM(C, window_size, K, d, Niter)\n",
    "    model.fit(tr_samples, tr_labels)\n",
    "    train_time = model.train_time\n",
    "    train_acc = model.accuracy(tr_samples, tr_labels)\n",
    "    start_test = timeit.default_timer()\n",
    "    test_acc = model.accuracy(te_samples, te_labels)\n",
    "    test_time = timeit.default_timer() - start_test\n",
    "\n",
    "    return {\n",
    "        \"method\": \"SVM\",\n",
    "        \"window_size\": window_size,\n",
    "        \"train_size\": train_size,\n",
    "        \"test_size\": test_size,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"train_time\": train_time,\n",
    "        \"test_time\": test_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaximumMarginMarkovNetworks:\n",
    "    def __init__(self, C, L, K, d, Niter):\n",
    "        self.C = C\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.Niter = Niter\n",
    "\n",
    "    def fit(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        problem = ThreeClassProblem(samples, labels, self.L, self.K, self.d)\n",
    "        start_train = timeit.default_timer()\n",
    "        self.weights = dlib.solve_structural_svm_problem(problem)\n",
    "        end_train = timeit.default_timer()\n",
    "        self.train_time = end_train - start_train\n",
    "        print(\"Training time elapsed:\", self.train_time, \"s\")\n",
    "        pickle.dump(self.weights, open('weights_m3n.obj', 'wb'))\n",
    "\n",
    "    def predict(self, samples):\n",
    "        predictions = []\n",
    "        problem = ThreeClassProblem(samples, [], self.L, self.K, self.d)\n",
    "        for idx, samp in enumerate(samples):\n",
    "            max1 = -1e10\n",
    "            max_scoring_label = [0] * self.L  # Initialize max_scoring_label for ICM search\n",
    "            for k in range(self.Niter):\n",
    "                for iL in range(self.L):  # Iterate over the window length\n",
    "                    for i in range(self.K):  # Change different label for the search of a structured label\n",
    "                        tmp_label = list(max_scoring_label)  # New a list to avoid modifying the max_scoring_label\n",
    "                        tmp_label[iL] = i  # Take turns to modify the structured label from left to right\n",
    "                        tmp_psi = problem.make_psi(samp, tmp_label)  # Make a new Psi for the guessed structured label\n",
    "                        score1 = dlib.dot(self.weights, tmp_psi)\n",
    "                        if max1 < score1:\n",
    "                            max1 = score1\n",
    "                            max_scoring_label[iL] = i\n",
    "            predictions.append(max_scoring_label)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, samples, labels):\n",
    "        predictions = self.predict(samples)\n",
    "        errCnt = sum([1 if predictions[i] != labels[i] else 0 for i in range(len(predictions))])\n",
    "        return 1.0 - float(errCnt) / float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad12def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run M3N experiments\n",
    "def run_mmmn_experiment(dataset, window_size, N, d, train_size, test_size):\n",
    "    labels, features = structurize(dataset, N, window_size)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    nplabels = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "    npsamples = np.hstack([np.ones((N, 1)), features])\n",
    "    K = len(le.classes_)\n",
    "\n",
    "    tr_labels = nplabels[:train_size].astype(int).tolist()\n",
    "    tr_samples = npsamples[:train_size].astype(int).tolist()\n",
    "    te_labels = nplabels[train_size:train_size + test_size].astype(int).tolist()\n",
    "    te_samples = npsamples[train_size:train_size + test_size].astype(int).tolist()\n",
    "\n",
    "    model = MaximumMarginMarkovNetworks(C, window_size, K, d, Niter)\n",
    "    model.fit(tr_samples, tr_labels)\n",
    "    train_time = model.train_time\n",
    "    train_acc = model.accuracy(tr_samples, tr_labels)\n",
    "    start_test = timeit.default_timer()\n",
    "    test_acc = model.accuracy(te_samples, te_labels)\n",
    "    test_time = timeit.default_timer() - start_test\n",
    "\n",
    "    return {\n",
    "        \"method\": \"M3N\",\n",
    "        \"window_size\": window_size,\n",
    "        \"train_size\": train_size,\n",
    "        \"test_size\": test_size,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"train_time\": train_time,\n",
    "        \"test_time\": test_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_crf(samples, labels):\n",
    "    data = []\n",
    "    for sample, label in zip(samples, labels):\n",
    "        features = []\n",
    "        for i in range(len(label)):\n",
    "            feature = {'bias': 1.0}\n",
    "            for j in range(d):\n",
    "                feature[f'x{i}_{j}'] = sample[i * d + j]\n",
    "            features.append((feature, str(label[i])))\n",
    "        data.append(features)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run CRF experiments\n",
    "def run_crf_experiment(dataset, window_size, N, d, train_size, test_size):\n",
    "    labels, features = structurize(dataset, N, window_size)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    nplabels = le.fit_transform(labels.ravel()).reshape(labels.shape)\n",
    "    npsamples = np.hstack([np.ones((N, 1)), features])\n",
    "    K = len(le.classes_)\n",
    "\n",
    "    tr_labels = nplabels[:train_size].astype(int).tolist()\n",
    "    tr_samples = npsamples[:train_size].astype(int).tolist()\n",
    "    te_labels = nplabels[train_size:train_size + test_size].astype(int).tolist()\n",
    "    te_samples = npsamples[train_size:train_size + test_size].astype(int).tolist()\n",
    "\n",
    "    tr_data = prepare_data_for_crf(tr_samples, tr_labels)\n",
    "    te_data = prepare_data_for_crf(te_samples, te_labels)\n",
    "    X_train = [x for x, y in tr_data]\n",
    "    y_train = [y for x, y in tr_data]\n",
    "    X_test = [x for x, y in te_data]\n",
    "    y_test = [y for x, y in te_data]\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=False\n",
    "    )\n",
    "    start_train = timeit.default_timer()\n",
    "    crf.fit(X_train, y_train)\n",
    "    train_time = timeit.default_timer() - start_train\n",
    "    y_pred_train = crf.predict(X_train)\n",
    "    train_acc = metrics.flat_accuracy_score(y_train, y_pred_train)\n",
    "    start_test = timeit.default_timer()\n",
    "    y_pred = crf.predict(X_test)\n",
    "    test_time = timeit.default_timer() - start_test\n",
    "    test_acc = metrics.flat_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"method\": \"CRF\",\n",
    "        \"window_size\": window_size,\n",
    "        \"train_size\": train_size,\n",
    "        \"test_size\": test_size,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"train_time\": train_time,\n",
    "        \"test_time\": test_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "N = 5000  # Number of examples\n",
    "d = 128  # Length of a feature\n",
    "Niter = 2  # Number of iterations for ICM search\n",
    "C = 1  # Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f96502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits\n",
    "splits = [(1000, 4000), (2500, 2500), (4000, 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different window sizes\n",
    "window_sizes = [2, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train_accuracies = []\n",
    "svm_test_accuracies = []\n",
    "svm_train_times = []\n",
    "svm_test_times = []\n",
    "\n",
    "m3n_train_accuracies = []\n",
    "m3n_test_accuracies = []\n",
    "m3n_train_times = []\n",
    "m3n_test_times = []\n",
    "\n",
    "crf_train_accuracies = []\n",
    "crf_test_accuracies = []\n",
    "crf_train_times = []\n",
    "crf_test_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b098be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_size, test_size in splits:\n",
    "    for window_size in window_sizes:\n",
    "        labels1, features1 = structurize(dataset1, N, window_size)\n",
    "        le1 = preprocessing.LabelEncoder()\n",
    "        nplabels1 = le1.fit_transform(labels1.ravel()).reshape(labels1.shape)\n",
    "        npsamples1 = np.hstack([np.ones((N, 1)), features1])\n",
    "        K1 = len(le1.classes_)\n",
    "\n",
    "        tr_labels = nplabels1[:train_size].astype(int).tolist()\n",
    "        tr_samples = npsamples1[:train_size].astype(int).tolist()\n",
    "        te_labels = nplabels1[train_size:train_size + test_size].astype(int).tolist()\n",
    "        te_samples = npsamples1[train_size:train_size + test_size].astype(int).tolist()\n",
    "\n",
    "        # Train and evaluate Structured SVM\n",
    "        structured_svm = StructuredSVM(C, window_size, K1, d, Niter)\n",
    "        structured_svm.fit(tr_samples, tr_labels)\n",
    "        svm_train_times.append(structured_svm.train_time)\n",
    "        svm_train_acc = structured_svm.accuracy(tr_samples, tr_labels)\n",
    "        svm_train_accuracies.append(svm_train_acc)\n",
    "\n",
    "        start_test = timeit.default_timer()\n",
    "        svm_test_acc = structured_svm.accuracy(te_samples, te_labels)\n",
    "        end_test = timeit.default_timer()\n",
    "        svm_test_times.append(end_test - start_test)\n",
    "        svm_test_accuracies.append(svm_test_acc)\n",
    "        print(f\"Training accuracy (SVM, window_size={window_size}, train_size={train_size})=\", svm_train_acc)\n",
    "        print(f\"Test accuracy (SVM, window_size={window_size}, test_size={test_size})=\", svm_test_acc)\n",
    "\n",
    "        # Train and evaluate M3N\n",
    "        m3n = MaximumMarginMarkovNetworks(C, window_size, K1, d, Niter)\n",
    "        m3n.fit(tr_samples, tr_labels)\n",
    "        m3n_train_times.append(m3n.train_time)\n",
    "        m3n_train_acc = m3n.accuracy(tr_samples, tr_labels)\n",
    "        m3n_train_accuracies.append(m3n_train_acc)\n",
    "\n",
    "        start_test = timeit.default_timer()\n",
    "        m3n_test_acc = m3n.accuracy(te_samples, te_labels)\n",
    "        end_test = timeit.default_timer()\n",
    "        m3n_test_times.append(end_test - start_test)\n",
    "        m3n_test_accuracies.append(m3n_test_acc)\n",
    "        print(f\"Training accuracy (M3N, window_size={window_size}, train_size={train_size})=\", m3n_train_acc)\n",
    "        print(f\"Test accuracy (M3N, window_size={window_size}, test_size={test_size})=\", m3n_test_acc)\n",
    "\n",
    "        # Prepare data for CRF\n",
    "        tr_data = prepare_data_for_crf(tr_samples, tr_labels)\n",
    "        te_data = prepare_data_for_crf(te_samples, te_labels)\n",
    "        X_train = [x for x, y in tr_data]\n",
    "        y_train = [y for x, y in tr_data]\n",
    "        X_test = [x for x, y in te_data]\n",
    "        y_test = [y for x, y in te_data]\n",
    "\n",
    "        # Train CRF model\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=False\n",
    "        )\n",
    "        start_train = timeit.default_timer()\n",
    "        crf.fit(X_train, y_train)\n",
    "        end_train = timeit.default_timer()\n",
    "        crf_train_times.append(end_train - start_train)\n",
    "        y_pred_train = crf.predict(X_train)\n",
    "        crf_train_acc = metrics.flat_accuracy_score(y_train, y_pred_train)\n",
    "        crf_train_accuracies.append(crf_train_acc)\n",
    "\n",
    "        start_test = timeit.default_timer()\n",
    "        y_pred = crf.predict(X_test)\n",
    "        end_test = timeit.default_timer()\n",
    "        crf_test_times.append(end_test - start_test)\n",
    "        crf_test_acc = metrics.flat_accuracy_score(y_test, y_pred)\n",
    "        crf_test_accuracies.append(crf_test_acc)\n",
    "        print(f\"Training accuracy (CRF, window_size={window_size}, train_size={train_size})=\", crf_train_acc)\n",
    "        print(f\"Test accuracy (CRF, window_size={window_size}, test_size={test_size})=\", crf_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ec1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot for different splits\n",
    "split_labels = [\"1000/4000\", \"2500/2500\", \"4000/1000\"]\n",
    "\n",
    "for i, (train_size, test_size) in enumerate(splits):\n",
    "    plt.subplot(3, 2, 2 * i + 1)\n",
    "    plt.plot(window_sizes, svm_train_accuracies[i * len(window_sizes):(i + 1) * len(window_sizes)], label='SVM Train Accuracy')\n",
    "    plt.plot(window_sizes, m3n_train_accuracies[i * len(window_sizes):(i + 1) * len(window_sizes)], label='M3N Train Accuracy')\n",
    "    plt.plot(window_sizes, crf_train_accuracies[i * len(window_sizes):(i + 1) * len(window_sizes)], label='CRF Train Accuracy')\n",
    "    plt.xlabel('Window Size')\n",
    "    plt.ylabel('Training Accuracy')\n",
    "    plt.title(f'Training Accuracy Comparison (Split: {split_labels[i]})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 2, 2 * i + 2)\n",
    "    plt.plot(window_sizes, svm_test_accuracies[i * len(window_sizes):(i + 1) * len(window_sizes)], label='SVM Test Accuracy')\n",
    "    plt.plot(window_sizes, m3n_test_accuracies[i * len(window_sizes):(i + 1) * len(window_sizes)], label='M3N Test Accuracy')\n",
    "    plt.plot(window_sizes, crf_test_accuracies[i * len(window_sizes):(i + 1) * len(window_sizes)], label='CRF Test Accuracy')\n",
    "    plt.xlabel('Window Size')\n",
    "    plt.ylabel('Testing Accuracy')\n",
    "    plt.title(f'Testing Accuracy Comparison (Split: {split_labels[i]})')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
